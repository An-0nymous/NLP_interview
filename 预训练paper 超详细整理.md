## 一、不同mask策略

## BERT-wwm：全词mask



## Spanbert

几何分布（Geometric distribution）是离散型概率分布。其中一种定义为：在n次[伯努利试验](https://baike.baidu.com/item/伯努利试验/238488)中，试验k次才得到第一次成功的机率。详细地说，是：前k-1次皆失败，第k次成功的概率

根据几何分布，先随机选择一段（span）的长度，之后再根据均匀分布随机选择这一段的起始位置，最后按照长度遮盖。文中使用几何分布取 p=0.2，最大长度只能是 10，利用此方案获得平均采样长度分布.通过采样，平均被遮盖长度是 3.8 个词的长度。

Span Boundary Objective 是该论文加入的新训练目标，希望被遮盖 Span 边界的词向量，能学习到 Span 的内容。或许作者想通过这个目标，让模型在一些需要 Span 的下游任务取得更好表现，结果表明也正如此。

具体做法是，在训练时取 Span 前后边界的两个词，值得指出，这两个词不在 Span 内，然后用这两个词向量加上 Span 中被遮盖掉词的位置向量，来预测原词。

详细做法是将词向量和位置向量拼接起来，过两层全连接层，很简单：

![img](file:////private/var/folders/fc/q0g7d2714jb7bdvgq3xytzsc0000gn/T/com.kingsoft.wpsoffice.mac/wps-yuqin/ksohtml/wpsVnq6Dw.png) 

# 二、引入常识

## 百度erine1

基于字特征输入 [一个是对分词工具有依赖，尤其是NER、新词等OOV问题，会比较影响模型效果。第二个是在预测的时候，如果是基于字的则预测结果标签集合较小，而如果是基于词的，明显标签空间会大很多，这很可能也会有劣势。而百度仍然采取字输入，但是Mask采取单词的方式，我觉得算是一种折中方案，包括N-gram，也算折中方案，能比较好的平衡两者，其实是挺好的]

ERNIE预训练的输入，利用内部词法分析工具对句对数据进行字、词、实体等不同粒度的切分，然后基于 tokenization.py 中的 CharTokenizer 对切分后的数据进行 token 化处理，得到明文的 token 序列及切分边界，然后将明文数据根据词典config/vocab.txt 映射为 id 数据，在训练过程中，根据切分边界对连续的 token 进行随机 mask 操作。

通过知识集成的增强表示(ERNIE)，它的思想其实和前面的Whole Word Masking非常类似，只是把Masking的整体从词多大到短语(phrase)和实体(entity)而已。需要用NLP的工具来识别phrase和entity。对于英文的phrase，可以使用chunking工具，而中文是百度自己的识别phrase的工具只用基本级别(中文的字)，测试集上的结果是76.8%，使用了phrase之后，能提高0.5%到77.3%，再加上entity之后可以提高到77.6%。 短语mask,实体mask

BERT 仍然存在以下缺点： Masked LM 的建模过程中是以字为单位的，这使得模型很难学习到语义知识单元的完整语义表示，而这一缺点对于中文的 BERT 预训练模型与应用而说尤为明显，文章举例如下：eg：对于乒 [mask] 球，清明上 [mask] 图，[mask] 颜六色这些词，BERT 模型通过字的搭配，很容易推测出掩码的字信息，但没有显式地对语义概念单元 (如乒乓球、清明上河图) 以及其对应的语义关系进行建模。 为啥把“知识图谱”引入Transformer是个好的改进方向呢？我们可以认为Bert的预训练阶段采取的语言模型任务，这算是通用的语言知识，胜在量大，但是因为是自监督的模式，虽然其实里面也大量包含了各种“知识图谱”中的知识，比如“太原是山西的省会”这种句子里的知识应该也能通过语言模型编码到TF参数里。但是毕竟不是专门学习这种知识，所以可能针对这种知识的编码能力不算太强，第一阶段：增大数据规模，增加数据多样性，增加数据质量。量大，质好，花样多。只要持续做这三个事情，感觉性能还有提升空间  GPT2

第二阶段：“知识图谱”的编码放到第二个阶段，比如拿到一个非常大的知识图谱，假设它是三元组表示<实体1，关系R，实体2>，要求Bert去做有监督的训练，比如可以输入<实体1，关系R>要求预测实体2，或者输入<实体1，实体2>，要求Bert预测它们的关系。等等，有不同的方法可以强迫Bert去学习知识图谱里的知识，如果这个图谱量级够大，那么这样的有监督地专门学习阶段学习效率应该是比让它去通过语言模型学习知识更有效



# 三、引入多任务学习

## 百度erine2

持续学习：不是训练所有任务，而是按顺序训练它们：

在任务1上进行训练

使用上一步中的参数，并在任务1、2上进行训练

使用上一步中的参数，并在任务1、2、3上进行训练，以此类推…

 我们是逐步学习而不是一次学习多个任务。之所以行之有效，是因为如果达到任务1的全局最小值，那么将两个损失函数加在一起时，与使用完全随机参数开始时相比，更有可能获得全局最小值

 缺陷：添加七大任务来进行词汇，句法，语义上信息的捕捉，但未做abation study 来验证到底哪个任务最有效

开源的 ERNIE 模型支持生成字级别的向量和句子级别的动态向量；不支持生成词级别的向量

##  MTDNN

对于语言模型来说，不同领域的文本相当于一个独立的task，而如果把这些task组合起来学习，那么就是multi-task学习。所特殊的是这些task都是同质的，即它们的目标函数都是一样的，所以可以统一学习。那么当增大数据集后，相当于模型在更多领域上进行了学习，即模型的泛化能力有了进一步的增强。

MTDNN多任务学习的优点主要有两个：

\1. 在标注数据较少的情况下，可以利用到其他相似任务的标注数据

\2. 减少针对特定任务的过拟合，起到正则化的作用

***\*但问题来了：\***

\1. 拿BERT的哪个编码表示？（CLS、SEP还是都拼起来？）

\2. [如何进行multi-task训练？](https://link.zhihu.com/?target=https://cloud.tencent.com/developer/news/192903)（是分别训练，还是一起训练？）

1.2 Task specific layers

Single Sentence Classification：采用[CLS]作为句子编码表示，softmax损失函数

Pairwise Text Similarity：采用[CLS]作为句子编码表示，sigmoid损失函数

Pairwise Text Classification：分别将第一句和第二句的词编码拼接，参考[SAN模型](https://link.zhihu.com/?target=https://arxiv.org/abs/1712.03556)迭代推理结果

Pairwise Ranking：采用[CLS]作为句子编码表示，sigmoid损失函数

1.3 模型训练

联合训练：作者将所有task的batch训练数据混合成数据集D，每次从D中拿出一个任务的batch进行训练。

相比于交替训练（先训练任务A再训练任务B），这样做的好处是避免偏向某个任务。

结论：多任务学习可以提升模型表现

BERT证明了在大规模无监督语料上进行预训练可以提升NLP任务的表现，MT-DNN则证明了在一定规模的有监督语料上，用多任务学习+预训练模型会带来更好的表现。可以说比BERT更进一步地解决了业界标注数据稀缺的情况。

 多任务学习具有更好的泛化性能：作者又在其他数据集上进行了实验，以验证MT-DNN模型在新任务中的表现，还是优于精挑BERT。因此MT-DNN具备更好的泛化性能。

***\*Next Sentence Prediction (NSP)\**** ***\*s\*******\*entence Order Prediction (SOP)\*******\*(albert)\****

# 四 引入多模态



# 五、压缩：参数压缩前提下 模型效果别差太多

## Albert

##### 1.将embedding的参数进行了因式分解

原始的BERT模型以及各种依据transformer来搞的预训练语言模型在输入的地方我们会发现它的E是等于H的，其中E就是embedding size，H就是hidden size，也就是transformer的输入输出维度。这就会导致一个问题，当我们的hidden size提升的时候，embedding size也需要提升，这就会导致我们的embedding matrix维度的提升。所以这里作者将E和H进行了解绑，具体的操作其实就是one-hot向量映射到一个低维度的空间，大小为E，然后再映射到一个高维度的空间，说白了就是先经过一个维度很低的embedding matrix，然后再经过一个高维度matrix把维度变到隐藏层的空间内，从而把参数量从O ( V × H ) 降低到了O ( V × E + E × H ) O(V×E+E×H)O(V×E+E×H)，当E<<H时参数量减少的很明显。

这使得embedding参数的维度从O(V×H)到了O(V×E + E×H), 当E远远小于H的时候更加明显。

##### 2.跨层的参数共享

##### Transformer中共享参数有多种方案，只共享全连接层，只共享attention层，ALBERT结合了上述两种方案，全连接层与attention层都进行参数共享，也就是说共享encoder内的所有参数，同样量级下的Transformer采用该方案后实际上效果是有下降的，但是参数量减少了很多，训练速度也提升了很多。

3、 抛弃了原来的NSP任务，现在使用**SOP**任务。

这里作者使用了一个新的loss，其实就是更改了原来BERT的一个子任务NSP, 原来NSP就是来预测下一个句子的，也就是一个句子是不是另一个句子的下一个句子。这个任务的问题出在训练数据上面，正例就是用的一个文档里面连续的两句话，但是负例使用的是不同文档里面的两句话。这就导致这个任务包含了主题预测在里面，而主题预测又要比两句话连续性的预测简单太多。新的方法使用了sentence-order prediction(SOP), 正例的构建和NSP是一样的，不过负例则是将两句话反过来。实验的结果也证明这种方式要比之前好很多。但是这个这里应该不是首创了，百度的ERNIE貌似也采用了一个这种的。

移除dropout

ALBERT在训练了100w步之后，模型依旧没有过拟合，于是乎作者果断移除了dropout，没想到对下游任务的效果竟然有一定的提升。这也是业界第一次发现dropout对大规模的预训练模型会造成负面影响

# 六、巨无霸：模型越来越大、参数越来越多

## GPT-2

其实相对于GPT1.0在模型结构和训练模式上并没有本质的区别，GPT2.0仅仅是加大了模型结构和训练语料的规模。GPT2.0使用了约 1000 万篇文章的数据集，文本集合达 40GB。这样训练出来的语言模型优势很明显，比使用专有数据集来的通用性更强，更能理解语言和知识逻辑，可以用于任意领域的下游任务。还有一个变化是，在GPT2.0中，OpenAI没有再强调Finetune，也就说，OpenAI认为，只要模型能力够强，语料够好覆盖面够广，Finetune的过程其实不是必要的。

## **RoBERTa** 

从模型上来说，RoBERTa基本没有什么太大创新，它在模型层面没有改变Google的Bert，改变的只是预训练的方法。主要是在BERT基础上做了几点调整： 

1）训练时间更长，batch size更大，训练数据更多；采用了 160G 的训练文本，而 BERT 仅使用 16G 的训练文本。更大的预训练数据、更多的训练步数

2）移除了next predict loss；

 3）动态调整Masking机制。

做动态Masking：原来Bert对每一个序列随机选择15%的Tokens替换成[MASK]，为了消除与下游任务的不匹配，还对这15%的Tokens进行（1）80%的时间替换成[MASK]；（2）10%的时间不变；（3）10%的时间替换成其他词。但整个训练过程，这15%的Tokens一旦被选择就不再改变，也就是说从一开始随机选择了这15%的Tokens，之后的N个epoch里都不再改变了。这就叫做静态Masking。

而RoBERTa一开始把预训练的数据复制10份，每一份都随机选择15%的Tokens进行Masking，也就是说，同样的一句话有10种不同的mask方式。然后每份数据都训练N/10个epoch。这就相当于在这N个epoch的训练中，每个序列的被mask的tokens是会变化的。

#  七、新的训练框架 

## ELECTRA

ELECTRA最主要的贡献是提出了新的预训练任务和框架，把生成式的Masked language model(MLM)预训练任务改成了判别式的Replaced token detection(RTD)任务，判断当前token是否被语言模型替换过。使用一个MLM的G-BERT来对输入句子进行更改，然后丢给D-BERT去判断哪个字被改过。输入句子经过生成器，输出改写过的句子，因为句子的字词是离散的，所以梯度在这里就断了，判别器的梯度无法传给生成器，于是生成器的训练目标还是MLM（作者在后文也验证了这种方法更好），判别器的目标是序列标注（判断每个token是真是假），两者同时训练，但判别器的梯度不会传给生成器，目标函数如下：

因为判别器的任务相对来说容易些，RTD loss相对MLM loss会很小，因此加上一个系数，作者训练时使用了50。

另外要注意的一点是，在优化判别器时计算了所有token上的loss，而以往计算BERT的MLM loss时会忽略没被mask的token。作者在后来的实验中也验证了在所有token上进行loss计算会提升效率和效果。

ELECTRA缺陷：

显存占用增多：之前都是训一个BERT，现在相当于两个，即使共享参数去训练，由于Adam等优化器的关系，需要保存的中间结果数量也是翻倍的。

多任务学习超参数调整：ELECTRA的loss由生成器和判别器分别构成，原文中给判别器loss的权重是50，因为二分类的loss会比V分类低很多。但这个数值太过绝对，最好是变成可学习的参数动态调整。

判别器的表示适合下游任务吗？

在BERT上进行MLM的训练，再放到下游任务是合理的，因为我们认为BERT的V分类任务给每个token生成了一个基于上下文的表示。可判别器的目标是2分类，也就是把token表示划分到两个空间中，使hidden space过早退化，遇到下游复杂任务时不足以生成丰富的表示。从Electra的设计来说，就是希望用更小的模型去更focus的学习判别任务，尤其是分类任务。所以序列标注、抽取、生成等不是那么“分类”的任务效果不好也是意料之内的

## Xlnet

理论上对于长度为T的序列***\*x\****，存在T!种排列方法，如果把 重新排列成 ，再采用AR为目标函数，则优化的似然为 。因为对于不同的排列方式，模型参数是共享的，所以模型最终可以学习到如何聚集所有位置的信息。操作上，由于计算复杂度的限制，不可能计算所有的序列排列，因此对于每个序列输入只采样一个排列方式。而且在实际训练时，不会打乱序列，而是通过mask矩阵实现permutation。作者特意强调，这样可以保持与finetune输入顺序的一致，不会存在pretrain-finetune差异。

Permutation Language Modeling：先给我们统一了之前语言模型的思想框架（AR or AE），再一个permutation把两者的优点结合起来，而且整体框架又回归到了AR，感觉生成模型的新SOTA指日可待。

Transformer-XL + Relative segment encoding：这个不是作者重点强调的，但却让我觉得很有用处，目前短文本的任务还好，文本一长难度就会上去，段落级甚至文章级，这两个操作让我看到了NLU在长文本上取得更大成果的可

看上去仍然是从左向右的输入和预测模式，但是其实内部已经引入了当前单词的下文信息呢？怎么能够在单词Ti的上文中Contenxt_before中揉入下文Context_after的内容呢？你可以想想。XLNet是这么做的，在预训练阶段，引入Permutation Language Model的训练目标。什么意思呢？就是说，比如包含单词Ti的当前输入的句子X，由顺序的几个单词构成，比如x1,x2,x3,x4四个单词顺序构成。我们假设，其中，要预测的单词Ti是x3，位置在Position 3，要想让它能够在上文Context_before中，也就是Position 1或者Position 2的位置看到Position 4的单词x4。可以这么做：假设我们固定住x3所在位置，就是它仍然在Position 3，之后随机排列组合句子中的4个单词，在随机排列组合后的各种可能里，再选择一部分作为模型预训练的输入X。比如随机排列组合后，抽取出x4,x2，x3,x1这一个排列组合作为模型的输入X。于是，x3就能同时看到上文x2，以及下文x4的内容了。这就是XLNet的基本思想，看上去仍然是个自回归的从左到右的语言模型，但是其实通过对句子中单词排列组合，把一部分Ti下文的单词排到Ti的上文位置中，于是，就看到了上文和下文，但是形式上看上去仍然是从左到右在预测后一个单词。

# **补充**

## GPT-1

通用预训练语言模型，是一种利用Transformer作为特征抽取器，基于语言模型进行训练的预训练语言模型。

语言模型通常构建为一句话的概率分布p(W)，这里的p(W)实际上反映的是W作为一个句子出现的概率。说成大白话，语言模型就是计算某个句子出现的概率。对于一个由T个词按顺序构成的句子，P(W)实际上求解的是字符串的联合概率，利用贝叶斯公式，链式分解如下：从上面可以看到，一个统计语言模型可以表示成，给定前面的的词，求后面一个词出现的条件概率。GPT中引入的是Transformer中的解码器部分

解码器与编码器的差异在于self-attention层上，解码器加了一层掩码，这是为了在自注意力计算的时候屏蔽了来自当前计算位置右边所有单词的信息。试想一下，在上述语言模型中，如果在预测下一个词时，已经知道下一个词的信息了，这不是作弊吗？这一点是与BERT这种双向结构不同的地方。

##  vanilla Transformer

基于Transformer提出了一种训练语言模型的方法（ https://arxiv.org/abs/1808.04444 ），来根据之前的字符预测片段中的下一个字符。例如，它使用x 1 , x 2 , . . . , x n − 1 x_1, x_2, ..., x_{n-1}x1,x2,...,xn−1预测字符x n x_nxn，而在x n x_nxn之后的序列则被mask掉。论文中使用64层模型，并仅限于处理 512个字符这种相对较短的输入，因此它将输入分成段，并分别从每个段中进行学习，如下图所示。 在测试阶段如需处理较长的输入，该模型会在每一步中将输入向右移动一个字符，以此实现对单个字符的预测。

但它仍有以下两个缺点：

a. 上下文长度受限：字符之间的最大依赖距离受输入长度的限制，模型看不到出现在几个句子之前的单词。
b. 上下文碎片：对于长度超过512个字符的文本，都是从头开始单独训练的。段与段之间没有上下文依赖性，会让训练效率低下，也会影响模型的性能。

##### 引入重用机制

为了突破使用固定长度片段带来的语境限制，这篇论文在Transformer体系结构中引入了重复机制。在训练期间，为模型处理下一个新的片段时，会缓存前一个片段计算的隐藏状态序列，并作为扩展语境重用。虽然梯度仍然保留在一个片段内，但这个额外的输入，允许网络利用历史信息，从而能够对长期依赖建模，并避免场景碎片。除了这些好处之外，重复机制还能够加快评估速度。在评估期间，可以重复使用来自先前片段的表征，而不是像Vanilla模型从头开始。在针对enwiki8数据集的实验中，Transformer-XL在评估过程中比Vanilla模型快1800倍。

## Transformer-XL

架构在vanilla Transformer的基础上引入了两点创新：循环机制（Recurrence Mechanism）和相对位置编码（Relative Positional Encoding），以克服vanilla Transformer的缺点。超长文本的场景下，XLNet相比其他bert系列的模型会有更好的性能(recurrent机制使其可捕获更长的上下文依赖关系)以及更快的训练与推理速度(memory单元中缓存了之前(一个或多个)段的隐状态信息，避免了重复计算)

 

\1. 引入循环机制

与vanilla Transformer的基本思路一样，Transformer-XL仍然是使用分段的方式进行建模，但其与vanilla Transformer的本质不同是在于引入了段与段之间的循环机制，使得当前段在建模的时候能够利用之前段的信息来实现长期依赖性。如下图所示：
![img](file:////private/var/folders/fc/q0g7d2714jb7bdvgq3xytzsc0000gn/T/com.kingsoft.wpsoffice.mac/wps-yuqin/ksohtml/wpsFukWam.jpg)
在训练阶段，处理后面的段时，每个隐藏层都会接收两个输入：

该段的前面隐藏层的输出，与vanilla Transformer相同（上图的灰色线）。

前面段的隐藏层的输出（上图的绿色线），可以使模型创建长期依赖关系。

这两个输入会被拼接，然后用于计算当前段的Key和Value矩阵。对于某个段的某一层的具体计算公式如下：
。

\2. 相对位置编码

在Transformer中，一个重要的地方在于其考虑了序列的位置信息。在分段的情况下，如果仅仅对于每个段仍直接使用Transformer中的位置编码，即每个不同段在同一个位置上的表示使用相同的位置编码，就会出现问题。比如，第i − 2 i-2i−2段和第i − 1 i-1i−1段的第一个位置将具有相同的位置编码，但它们对于第i ii段的建模重要性显然并不相同（例如第i − 2 i-2i−2段中的第一个位置重要性可能要低一些）。因此，需要对这种位置进行区分。

论文对于这个问题，提出了一种新的位置编码的方式，即会根据词之间的相对距离而非像Transformer中的绝对位置进行编码。在Transformer中，第一层的计算查询q i T q_i^TqiT和键k j k_jkj之间的attention分数的方式为：
![img](file:////private/var/folders/fc/q0g7d2714jb7bdvgq3xytzsc0000gn/T/com.kingsoft.wpsoffice.mac/wps-yuqin/ksohtml/wps8n6Cvp.jpg)





  

